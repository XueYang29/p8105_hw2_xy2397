---
title: "p8105_hw2_xy2397"
author: "Xue Yang"
date: "9/27/2018"
output:
    github_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

library(tidyverse)
library(ggridges)
```



# Problem 1

**Read and clean the data**

Read and clean the data; retain line, station name, station latitude / longitude, routes served, entry, vending, entrance type, and ADA compliance. Convert the entry variable from character (YES vs NO) to a logical variable (the ifelse or recode function may be useful).

```{r import_and_maniqulate_data}
# read the csv file through a relative path
# clean up variable names
# select variables following the requirement
# convert the variables from character to logical

NYC_transit_data = 
  read_csv(file = "./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>%
  janitor::clean_names() %>% 
  select(line:route11, entry, vending, entrance_type, ada) %>%
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))


```

**Short paragraph about the dataset**

Write a short paragraph about this dataset – explain briefly what variables the dataset contains, describe your data cleaning steps so far, and give the dimension (rows x columns) of the resulting dataset. Are these data tidy?

This dataset "NYC_Transit_Subway_Entrance_And_Exit_Data" (the original dataset before cleaning) contains variables: division, line, station name, station latitude, station longitude, station location (station latitude, station longitude), routes served from 1 to 11, entrance Type (stair, door, elevator, escalator or easement), entry (yes or no), entrance latitude, entrance longitude, entrance location (entrance latitude, entrance longitude), exit only (whether or not), vending (yes or no), staffing (full, none or part), staff hours, ADA (ADA compliant or not), ADA notes, free crossover (true or false), North South Street, East West Street, corner(NW, NE, SW or SE).

I clean the data and rename the data "NYC_transit_data" from the following step:

* Import data: I import the data using relative path.

* Clean variable name: using clean_names function from janitor package to clean up variable names and convert them to lower snake case after importing data.

* Select variables: only retain the following variables: line, station name, station latitude / longitude, routes served, entry, vending, entrance type, and ADA compliance.

* Covert variable: convert the entry variable from character (YES vs NO) to a logical variable (TRUE vs FALSE).

Then the dimension of dataset after cleaning "NYC_transit_data" is `r nrow(NYC_transit_data)` rows x `r (ncol(NYC_transit_data))` columns.

These data is not tidy, since the data provided includes columns "route1" to "route11", which list all routes served by a station entrance/exit, which is not a "tidy" format.

**Answer the questions using the data**

•	How many distinct stations are there? Note that stations are identified both by name and by line (e.g. 125th St A/B/C/D; 125st 1; 125st 4/5); the distinct function may be useful here.

```{r}
# filter the data by distinct stations
distinct_station = 
  distinct(NYC_transit_data, line, station_name, .keep_all = TRUE)

# calculate the number of distinct stations
nrow(distinct_station)
```

So there are 465 distinct stations.

•	How many stations are ADA compliant?

```{r}
# filter the distinct stations that are ADA compliant
ADA_compliant =  filter(distinct_station, ada == "TRUE")

# calculate the number of ADA compliant distinct stations
nrow(ADA_compliant)
```


•	What proportion of station entrances / exits without vending allow entrance?

Since this questions ask about the station entrances/ exits, so I use the original data instead of distinct stations data to filter, since each distinct station may have some different entrances / exits.

```{r}
# filter the station entrances/ exits without vending from original data
no_vending = filter(NYC_transit_data, vending == "NO")

# filter which entrances/ exits without vending that allow extrance
allow_entrance = filter(no_vending, entry == "TRUE")

# calculate the proportion
nrow(allow_entrance)/nrow(no_vending)
```

Reformat data so that route number and route name are distinct variables. How many distinct stations serve the A train? Of the stations that serve the A train, how many are ADA compliant?

```{r}
# reformat data to distinct the route number and route name variables 
# distinct the stations by line and name
# filter the distinct stations that serve A train
# calculate the number of distinct stations that serve A train

NYC_transit_data %>% 
  gather(key = "route_number", value = "route_name", route1:route11) %>%
  distinct(line, station_name, .keep_all = TRUE) %>% 
  filter(route_name == "A") %>%
  nrow()
```
So there are 60 distinct stations serve the A train.

```{r}
# reformat data to distinct the route number and route name variables 
# distinct the stations by line and name
# filter the distinct stations that serve A train
# calculate the number of distinct stations that serve A train that are ADA complaint

NYC_transit_data %>% 
  gather(key = "route_number", value = "route_name", route1:route11) %>%
  distinct(line, station_name, .keep_all = TRUE) %>% 
  filter(route_name == "A") %>% 
  filter(ada == "TRUE") %>% 
  nrow()
```
So of the stations that serve the A train, 17 of them are ADA compliant.

# Problem 2

**Read and clean the Mr. Trash Wheel sheet**

•	specify the sheet in the Excel file and to omit columns containing notes (using the range argument and cell_cols()function)
•	use reasonable variable names
•	omit rows that do not include dumpster-specific data
•	rounds the number of sports balls to the nearest integer and converts the result to an integer variable (using as.integer)

```{r}
Mr.Trash_Wheel_data = 
  readxl::read_excel(path = "./data/HealthyHarborWaterWheelTotals2017-9-26.xlsx",
                     sheet = "Mr. Trash Wheel",
                     range = cellranger::cell_cols(c("A:N"))) %>% 
  janitor::clean_names() %>% 
  filter(!is.na(dumpster)) %>% 
  mutate(., sports_balls = as.integer(sports_balls)) 
```


**Read and clean precipitation data for 2016 and 2017**

For each, omit rows without precipitation data and add a variable year. Next, combine datasets and convert month to a character variable (the variable month.name is built into R and should be useful).

```{r}
precipitation_2016_data = 
  readxl::read_excel(path = "./data/HealthyHarborWaterWheelTotals2017-9-26.xlsx",
                     sheet = "2016 Precipitation",
                     range = "A2:B15") %>% 
  janitor::clean_names() %>% 
  filter(!is.na(month)) %>% 
  mutate(year = "2016")

```

```{r}
precipitation_2017_data = 
  readxl::read_excel(path = "./data/HealthyHarborWaterWheelTotals2017-9-26.xlsx",
                     sheet = "2017 Precipitation",
                     range = "A2:B15") %>% 
  janitor::clean_names() %>% 
  filter(!is.na(month)) %>% 
  mutate(year = "2017") 
```

```{r}
precipitation_data = 
  bind_rows(precipitation_2016_data, precipitation_2017_data) %>% 
  mutate(., month = month.name[month])
precipitation_data 
```


**A paragraph about these data**

you are encouraged to use inline R. Be sure to note the number of observations in both resulting datasets, and give examples of key variables. For available data, what was the total precipitation in 2017? What was the median number of sports balls in a dumpster in 2016?

From the Mr. Trash Wheel sheet, there are 215 observations of dumpsters. The dataset contains variables including month, year, data, weight(tons), volume (cubic yards), plastic bottles, polystyrene,	cigarette butts, glass Bottles, grocery bags, chip bags, sports balls and homes powered. 

From the 2016 and 2017 precipitation sheets, there are 12 and 8 observations respectively. The datasets contains variables including month and monthly total precipitation.

For available data, the total precipitiation in 2017 is `r (sum(filter(precipitation_2017_data, !is.na(total))$total))`, the median number of sports balls in a dumpster in 2016 is `r (median(filter(Mr.Trash_Wheel_data, year == "2016")$sports_balls))`

## Problem 3

**Load the data**

```{r}
library(p8105.datasets)
```

**Read and clean the data**

For this question:
•	format the data to use appropriate variable names;
•	focus on the “Overall Health” topic
•	exclude variables for class, topic, question, sample size, and everything from lower confidence limit to GeoLocation
•	structure data so that responses (excellent to poor) are variables taking the value of Data_value
•	create a new variable showing the proportion of responses that were “Excellent” or “Very Good”

```{r}
brfss = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(., topic == "Overall Health") %>% 
  select(., -class, -topic, -question, -sample_size, -(confidence_limit_low:geo_location)) %>%
  spread(key = "response", value = "data_value") %>% 
  janitor::clean_names() %>% 
  mutate(excellent_verygood_proportion = (excellent + very_good)/(excellent + very_good + good + fair + poor)) 
```

Using this dataset, do or answer the following:

•	How many unique locations are included in the dataset? Is every state represented? What state is observed the most?

```{r}
length(unique(brfss$locationdesc))
length(unique(brfss$locationabbr))
```


There are 404 unique location included in the dataset, and every state represented, since there is totally 51 states in the US, and the unique states included in the dataset is exactly 51.

```{r}
table(brfss$locationabbr)
max(table(brfss$locationabbr))
```

NJ is observed the most.

•	In 2002, what is the median of the “Excellent” response value?

```{r}
brfss_2002 = filter(brfss, year == "2002", !is.na(excellent))

median(brfss_2002$excellent)
```

•	Make a histogram of “Excellent” response values in the year 2002.
```{r}
ggplot(brfss_2002, aes(x = excellent)) +
  geom_histogram() +
  labs(
    title = "Excellent response values in 2002",
    x = "response value",
    y = "frequence",
    caption = "Data from the p8105.datasets package"
  ) 
```

•	Make a scatterplot showing the proportion of “Excellent” response values in New York County and Queens County (both in NY State) in each year from 2002 to 2010.

```{r}
brfss %>% 
  filter(locationdesc %in% c("NY - New York County", "NY - Queens County")) %>% 
  mutate(excellent_prop = (excellent)/(excellent + very_good + good + fair + poor))  %>% 
  ggplot(aes(x = year, y = excellent_prop , color = locationdesc )) +
  geom_point(alpha = 0.5) +
  labs(
    title = "Proportion of Excellent response values from 2002 to 2010",
    x = "Year",
    y = "Proportion of excellent response values ",
    caption = "Data from the p8105.datasets package"
  ) +
scale_color_hue(
    name = "Location",
  ) +
  theme(legend.position = "bottom")
```

